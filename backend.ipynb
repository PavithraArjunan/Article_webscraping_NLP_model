{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ04S/WVKVjtT2Py0b61nN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavithraArjunan/Article_webscraping_NLP_model/blob/main/backend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "EpEVyVN9E3nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2ufqs6Iuiw1tlCUd5HPma4X2rYJ_2Fv2BUzm2Lo36Pe9uWAM2"
      ],
      "metadata": {
        "id": "yVHez9OGFfXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gtts"
      ],
      "metadata": {
        "id": "f5P4NtkwfyvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deep_translator"
      ],
      "metadata": {
        "id": "CFVQWvt8fy3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask_cors"
      ],
      "metadata": {
        "id": "fXhayfRDfUDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, jsonify, request\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import spacy\n",
        "from gtts import gTTS\n",
        "from deep_translator import GoogleTranslator\n",
        "import os\n",
        "\n",
        "# Load spaCy NLP model for topic extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Flask app initialization\n",
        "app = Flask(__name__)\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\n",
        "    \"https://apnews.com/article/tesla-sales-2024-drop-electric-vehicles-69af17c4e606625694af8293db25b2f3\",\n",
        "    \"https://www.theweek.in/news/biz-tech/2025/02/25/without-low-cost-model-teslas-potential-entry-to-india-unlikely-to-hurt-indian-car-makers-say-analysts.html\",\n",
        "    \"https://apnews.com/article/cybertruck-recall-tesla-elon-musk-nhtsa-8c517e21aa1119d74b9db39f6aca01b7\",\n",
        "    \"https://www.news18.com/business/andhra-pradesh-makes-pitch-again-to-attract-teslas-manufacturing-plant-report-9236626.html\",\n",
        "    \"https://www.motorzest.com/2015/03/mahindra-two-wheeler-sales-down-537-in.html\",\n",
        "    \"https://www.prnewswire.com/in/news-releases/mahindra-first-choice-wheels-mfcwl-raises-15-million-from-san-francisco-based-valiant-capital-497010601.html\",\n",
        "    \"https://www.rediff.com/business/report/tata-mahindra-will-not-allow-tesla-to-dominate/20250228.htm\",\n",
        "    \"https://www.etnownews.com/companies/tata-motors-board-meeting-result-big-announcement-ahead-of-demerger-2-lakh-ncds-of-rs-100000-face-value-article-119216849\",\n",
        "    \"https://www.pbs.org/newshour/nation/nissan-and-honda-to-attempt-a-merger-that-would-create-the-worlds-no-3-automaker\",\n",
        "    \"https://www.indiainfoline.com/news/markets/tata-motors-and-mahindra-bet-big-on-electric-suvs-aiming-to-challenge-global-players\"\n",
        "]\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "# Company Keywords Mapping\n",
        "company_keywords = {\n",
        "    \"Tesla\": [\"tesla\", \"elon musk\", \"cybertruck\"],\n",
        "    \"Maruti Suzuki\": [\"maruti\", \"suzuki\"],\n",
        "    \"Mahindra\": [\"mahindra\", \"xuv700\", \"thar\"],\n",
        "    \"Tata Motors\": [\"tata\", \"jaguar\", \"land rover\"],\n",
        "    \"Nissan-Honda\": [\"nissan\", \"honda\"]\n",
        "}\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Perform sentiment analysis and return polarity label.\"\"\"\n",
        "    score = TextBlob(text).sentiment.polarity\n",
        "    if score > 0.5:\n",
        "        return \"Very Positive\"\n",
        "    elif score > 0:\n",
        "        return \"Positive\"\n",
        "    elif score == 0:\n",
        "        return \"Neutral\"\n",
        "    elif score > -0.5:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Very Negative\"\n",
        "\n",
        "def identify_company(title, summary):\n",
        "    \"\"\"Identify company based on keywords in the title/summary.\"\"\"\n",
        "    combined_text = f\"{title} {summary}\".lower()\n",
        "    for company, keywords in company_keywords.items():\n",
        "        if any(keyword in combined_text for keyword in keywords):\n",
        "            return company\n",
        "    return \"Other\"\n",
        "\n",
        "def extract_topics(summary):\n",
        "    \"\"\"Extract relevant topics dynamically using NLP.\"\"\"\n",
        "    doc = nlp(summary)\n",
        "    topics = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\", \"EVENT\"]]\n",
        "    return list(set(topics))  # Remove duplicates\n",
        "\n",
        "def scrape_article(url):\n",
        "    \"\"\"Scrape article title, summary, and analyze sentiment.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"❌ Failed to fetch {url}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        title = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"No title found\"\n",
        "        summary_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        summary = summary_tag[\"content\"].strip() if summary_tag else \"No summary found\"\n",
        "        sentiment = analyze_sentiment(summary)\n",
        "        company = identify_company(title, summary)\n",
        "        topics = extract_topics(summary)\n",
        "\n",
        "        return {\"Title\": title, \"Summary\": summary, \"Sentiment\": sentiment, \"Topics\": topics, \"Company\": company}\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Scrape all URLs\n",
        "scraped_articles = [scrape_article(url) for url in urls]\n",
        "articles = [article for article in scraped_articles if article]\n",
        "\n",
        "# Organize articles by company\n",
        "company_data = defaultdict(lambda: {\"Articles\": [], \"Sentiment Distribution\": defaultdict(int)})\n",
        "\n",
        "for article in articles:\n",
        "    company = article[\"Company\"]\n",
        "    company_data[company][\"Articles\"].append(article)\n",
        "    company_data[company][\"Sentiment Distribution\"][article[\"Sentiment\"]] += 1\n",
        "\n",
        "# Generate Comparative Sentiment Analysis\n",
        "for company, data in company_data.items():\n",
        "    articles = data[\"Articles\"]\n",
        "    comparisons = []\n",
        "    topic_overlap = {\"Common Topics\": [], \"Unique Topics in Articles\": []}\n",
        "\n",
        "    for i in range(len(articles) - 1):\n",
        "        article1, article2 = articles[i], articles[i + 1]\n",
        "        comparison = {\n",
        "            \"Comparison\": f\"Article {i+1} discusses {article1['Topics']} while Article {i+2} covers {article2['Topics']}\",\n",
        "            \"Impact\": \"Shows difference in focus across articles.\"\n",
        "        }\n",
        "        comparisons.append(comparison)\n",
        "        topic_overlap[\"Common Topics\"] = list(set(article1[\"Topics\"]) & set(article2[\"Topics\"]))\n",
        "        topic_overlap[\"Unique Topics in Articles\"].append({\n",
        "            f\"Unique Topics in Article {i+1}\": list(set(article1[\"Topics\"]) - set(article2[\"Topics\"])) ,\n",
        "            f\"Unique Topics in Article {i+2}\": list(set(article2[\"Topics\"]) - set(article1[\"Topics\"]))\n",
        "        })\n",
        "\n",
        "    data[\"Comparative Sentiment Score\"] = {\n",
        "        \"Sentiment Distribution\": dict(data[\"Sentiment Distribution\"]),\n",
        "        \"Coverage Differences\": comparisons,\n",
        "        \"Topic Overlap\": topic_overlap\n",
        "    }\n",
        "    sentiment_summary = \"positive\" if data[\"Sentiment Distribution\"][\"Positive\"] > data[\"Sentiment Distribution\"][\"Negative\"] else \"negative\"\n",
        "    data[\"Final Sentiment Analysis\"] = f\"Company’s latest news coverage leans towards {sentiment_summary}.\"\n",
        "\n",
        "# Convert sets to lists before JSON serialization\n",
        "output_data = {\n",
        "    company: {\n",
        "        **data,\n",
        "        \"Comparative Sentiment Score\": {\n",
        "            **data[\"Comparative Sentiment Score\"],\n",
        "            \"Topic Overlap\": {k: list(v) if isinstance(v, set) else v for k, v in data[\"Comparative Sentiment Score\"][\"Topic Overlap\"].items()}\n",
        "        }\n",
        "    }\n",
        "    for company, data in company_data.items()\n",
        "}\n",
        "\n",
        "# Function to convert text to Hindi speech\n",
        "def text_to_speech(text, filename=\"output.mp3\"):\n",
        "    translated_text = GoogleTranslator(source=\"auto\", target=\"hi\").translate(text)\n",
        "    tts = gTTS(translated_text, lang=\"hi\")\n",
        "\n",
        "    # Ensure static/audio directory exists\n",
        "    audio_folder = \"static/audio\"\n",
        "    os.makedirs(audio_folder, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(audio_folder, filename)\n",
        "    tts.save(file_path)\n",
        "\n",
        "    print(f\"✅ Audio saved at {file_path}\")\n",
        "    return file_path  # Return the correct file path\n",
        "\n",
        "\n",
        "# Extract text and generate audio for a company\n",
        "def generate_audio_for_company(company_data):\n",
        "    audio_files = {}\n",
        "    for company, data in company_data.items():\n",
        "        text_content = f\"{company} की ताज़ा खबरें:\\n\"\n",
        "        for article in data[\"Articles\"]:\n",
        "            text_content += f\"\\nशीर्षक: {article['Title']}\\nसारांश: {article['Summary']}\\n\"\n",
        "        for comparison in data[\"Comparative Sentiment Score\"][\"Coverage Differences\"]:\n",
        "            text_content += f\"\\nतुलना: {comparison['Comparison']}\\nप्रभाव: {comparison['Impact']}\\n\"\n",
        "\n",
        "        audio_filename = f\"{company}_news.mp3\"\n",
        "        text_to_speech(text_content, audio_filename)\n",
        "        audio_files[company] = os.path.abspath(audio_filename)  # Store file path\n",
        "    return audio_files\n",
        "\n",
        "\n",
        "# Define route to fetch company data\n",
        "@app.route('/get_company_data', methods=['GET'])\n",
        "def get_company_data():\n",
        "    user_input = request.args.get('company_name', '').strip().lower()\n",
        "    company_found = None\n",
        "\n",
        "    for company in company_data.keys():\n",
        "        if company.lower() == user_input:\n",
        "            company_found = company\n",
        "            break\n",
        "\n",
        "    if company_found:\n",
        "        # Generate the audio only if it does not already exist\n",
        "        if \"audio_file\" not in company_data[company_found]:\n",
        "            audio_files = generate_audio_for_company({company_found: company_data[company_found]})\n",
        "            # company_data[company_found][\"audio_file\"] = audio_files.get(company_found, None)\n",
        "            if company_found in audio_files:\n",
        "              audio_url = f\"{request.host_url}static/audio/{company_found}_news.mp3\"\n",
        "              company_data[company_found][\"audio_file\"] = audio_url\n",
        "\n",
        "        return jsonify({\n",
        "            \"company_data\": output_data[company_found],\n",
        "            \"audio_file\": company_data[company_found].get(\"audio_file\", None)  # Send stored file path\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        return jsonify({\"error\": \"No data found for the entered company. Please check the spelling.\"})\n",
        "\n",
        "\n",
        "# Start Flask app\n",
        "if __name__ == '__main__':\n",
        "    # Set up ngrok tunnel for Flask app\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * Flask app is running on {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "oGG96CdiqMKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}